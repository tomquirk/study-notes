## CNN
### Dropout
- fastai dropouts out twice in final layers by default

  - can turn it off by setting dropout to 0 and passsing empty list to fully connected layer param

- might need more dropout for larger architectures

- your only goal is to reduce validation loss; if that means overfitting a bit, so be it. You'll need to learn how much overfitting is okay for your problem

-can pass different dropout per layer

## Structured data
- some things, although continuous, may be better off as categorical (like day of week)
  - good rule of thumb: if floating point: continuous
  - consider binning
- fastai.proc_df
  - handles missing values
  - scales (neural nets like vals between 0-1)
- do lrfind

### embeddings
- a "distributed representation"
  - represent 1 category with multiple floats
- representing categorical vars in NN
- start out with integer between 0 and (max)
- when we create learner, we specify the embedding sizes for each cat_var
  - min(50, n+1//2)


## NLP (1:31:00)
- language modelling: model that can predict the next word of a given sentence
- *sentiment analysis* - train a language model, then fine-tune it for classification
- use `spacy` for tokenization
- use `torchtext` to preprocess 
- common in NLP to use `min_freq` - how many times does a given word have to appear before we consider it a word? (otherwise just unknown)
- vocabulary - what is the list of unique words that appear in this text

1. Join every token in the DS together on a single line
2. split it up into batches. `batch_size` is how many batches (e.g. 64).
3. Stack the batches vertically. E.g. if we started with DS length 64mil, we now have 1mil * 64 matrix
4. `back_prop_through_time` - how many words are processing at a time in each row of the mini-batch.
  - how long a sentence do we stick on the GPU at once?
  - basically, take the first `bptt` words from each batch
  - torch text will randomly change this number

- In NLP, theres 1 variable, "word" and we treat it as categorical - each word is a category!
- Embedding matrix is therefore [num_tokens] * [embedding_size] - each word gets set of embeddings
  - embedding size for a word probably [50, 600]
- Need to tweak Adam optimizer (1:59:28)
- Fastai uses AWD LSTM language model
- you can get pretrained embedding matricies - Word2Vec and GLOVE. Using them might not be worth it though

- to use pretrained (language) model on sentiment analysis, need to use EXACTLY the same vocab.
- basically
  1. create a "learner" object for sentiment analysis. It will have columns "label" and "text" for example.
  2. load in the pretrained model (load_encoder)